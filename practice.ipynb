{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/Tara8082/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Python support modules\n",
    "import re\n",
    "import string\n",
    "import datetime\n",
    "import pickle \n",
    "from collections import Counter\n",
    "\n",
    "# Spacy\n",
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "#import en_core_web_sm\n",
    "from spacy.pipeline import SentenceSegmenter\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import words, stopwords, wordnet\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Helper functions in py file\n",
    "from preprocessing_headlines import cleaned_headline\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Practice notebook\n",
    "\n",
    "\n",
    "data = pd.read_csv('/Users/Tara8082/GIT/ProjectGIT/Project_4/miscellaneous_support/final_processed_headlines.csv')\n",
    "pd.set_option('max_colwidth', 100)\n",
    "\n",
    "# Preprocessing date column, converting to date time, dropping old date column\n",
    "\n",
    "# data.date=data.date.str[:10]\n",
    "# data['date'] = data['date'].str.replace('-','/')\n",
    "# data['datetime'] = pd.to_datetime(data['date'], format=\"%Y/%m/%d\")\n",
    "# data.drop('date', axis=1, inplace=True)  \n",
    "# data.head()\n",
    "\n",
    "headlines = data[['content']].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 33.1 s, sys: 643 ms, total: 33.7 s\nWall time: 36.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "headlines['clean_content'] = headlines['content'].map(cleaned_headline).astype(str)\n",
    "\n",
    "# if word is not proper noun, lower case it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                                                               content  \\\n",
       "0  Biden leads Trump among Hispanic voters, 62% to 29%, a WSJ/NBC/Telemundo poll shows https://t.co...   \n",
       "1  “We can’t keep up with the laundry.” Covid-19 has turned the tourism industry upside down, creat...   \n",
       "2  A large English study showed the number of people with Covid-19 antibodies declined significantl...   \n",
       "3  The leaders of Microsoft, Coca-Cola, American Airlines and other companies discuss how business ...   \n",
       "4  After seven months of isolation, the pull of getting together is strong. But with Covid-19 hospi...   \n",
       "\n",
       "                                                                                         clean_content  \n",
       "0                            Biden leads Trump among Hispanic voters to a WSJ NBC Telemundo poll shows  \n",
       "1   We can t keep up with the laundry   Covid has turned the tourism industry upside down creating ...  \n",
       "2  A large English study showed the number of people with Covid antibodies declined significantly o...  \n",
       "3  The leaders of Microsoft Coca Cola American Airlines and other companies discuss how business is...  \n",
       "4  After seven months of isolation the pull of getting together is strong But with Covid hospitaliz...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>content</th>\n      <th>clean_content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Biden leads Trump among Hispanic voters, 62% to 29%, a WSJ/NBC/Telemundo poll shows https://t.co...</td>\n      <td>Biden leads Trump among Hispanic voters to a WSJ NBC Telemundo poll shows</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>“We can’t keep up with the laundry.” Covid-19 has turned the tourism industry upside down, creat...</td>\n      <td>We can t keep up with the laundry   Covid has turned the tourism industry upside down creating ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A large English study showed the number of people with Covid-19 antibodies declined significantl...</td>\n      <td>A large English study showed the number of people with Covid antibodies declined significantly o...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>The leaders of Microsoft, Coca-Cola, American Airlines and other companies discuss how business ...</td>\n      <td>The leaders of Microsoft Coca Cola American Airlines and other companies discuss how business is...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>After seven months of isolation, the pull of getting together is strong. But with Covid-19 hospi...</td>\n      <td>After seven months of isolation the pull of getting together is strong But with Covid hospitaliz...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "headlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "There are 541 stop words in this list.\n"
     ]
    }
   ],
   "source": [
    "standard_stop_words = stopwords.words(\"english\")\n",
    "\n",
    "# Open list of Twitter stop words\n",
    "\n",
    "with open('stopwords.txt') as fp:\n",
    "    # 1. iterate over file line-by-line\n",
    "    # 2. strip line of newline symbols\n",
    "    # 3. split line by spaces into list (of number strings)\n",
    "    # 4. convert to string\n",
    "    # 5. convert map object to list\n",
    "    stopwords_data = [list(map(str, line.strip().split(' '))) for line in fp]\n",
    "\n",
    "# Combining standard list and imported list of stop words\n",
    "\n",
    "flat_stopwords_data = [item for sublist in stopwords_data for item in sublist]\n",
    "stopwords = standard_stop_words + flat_stopwords_data\n",
    "stopwords_set = set(stopwords) \n",
    "\n",
    "#adding 'breaking' to list of stop words\n",
    "stopwords_set.update(['breaking', 'report', 'seven', 'people', 'happening', 'now', 'wsjbooks', 'wsj', 'wsjopinion', 'zyahna', 'zyairr', 'zydeco', 'zymere', 'zymergen', 'zynga', 'zz', 'wsj', 'nbc', 'zziya', 'zzz', 'zzzs', 'zwaan', 'zwack', 'zwang', 'zweibel', 'zweli', 'zwetsloot', 'zwickau','zwift', 'zwillinger', 'zwoolfe', 'zurfi', 'zuri', 'zury', 'zushaelinson', 'zutors','zuurbekom', 'zuzana', 'zverev', 'zvyagintsev', 'morning', 'brief', 'briefing', 'page', 'story', 'rundown', 'update', 'even', 'day', 'evening', 'publish', 'edition', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday'])\n",
    "\n",
    "#stopwords_set included standard stop words, twitter words, and common words to media accounts\n",
    "print(\"There are {} stop words in this list.\".format(len(stopwords_set)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['ner', 'sentencizer']\n"
     ]
    }
   ],
   "source": [
    "disabled_components = ['tagger', 'parser']\n",
    "\n",
    "nlp = spacy.load('en', disable=disabled_components)  # need to fix english model, need smaller english model\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "source": [
    "## SPACY PIPELINE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_nlp_pipe(doc):\n",
    "    '''\n",
    "    function that normalizes the text (lemmatization) if text is alpha and not in list of stopwords\n",
    "    '''\n",
    "    lemma_list = [str(token.lemma_) for token in doc\n",
    "                 if token.is_alpha and token.text not in stopwords_set]\n",
    "    lem_string = \" \".join(lemma_list)\n",
    "    return lem_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity(doc):\n",
    "    if doc.ents:\n",
    "        for entity in doc.ents:\n",
    "            print(entity.text, entity.start_char, entity.end_char, entity.label_)\n",
    "    else:\n",
    "        print(\"Not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessed_pipe(texts, batch_size=100):\n",
    "    preprocessed_pipe = []\n",
    "    for doc in nlp.pipe(texts, batch_size=batch_size):\n",
    "        if doc.ents:\n",
    "            preprocessed_pipe.append(lemmatize_nlp_pipe(entity(doc)))\n",
    "        #else:\n",
    "            #preprocessed_pipe.append(lemmatize_nlp_pipe(entity(doc)))\n",
    "    return preprocessed_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Hispanic 24 32 NORP\nWSJ 45 48 ORG\nNBC Telemundo 49 62 ORG\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-2fb9dc6876cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mheadlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessed_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheadlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-57-8bb17f4db40c>\u001b[0m in \u001b[0;36mpreprocessed_pipe\u001b[0;34m(texts, batch_size)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             \u001b[0mpreprocessed_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatize_nlp_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;31m#else:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;31m#preprocessed_pipe.append(lemmatize_nlp_pipe(entity(doc)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-8325134f3c36>\u001b[0m in \u001b[0;36mlemmatize_nlp_pipe\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfunction\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mnormalizes\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlemmatization\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     '''\n\u001b[0;32m----> 5\u001b[0;31m     lemma_list = [str(token.lemma_) for token in doc\n\u001b[0m\u001b[1;32m      6\u001b[0m                  if token.is_alpha and token.text not in stopwords_set]\n\u001b[1;32m      7\u001b[0m     \u001b[0mlem_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemma_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "headlines['test'] = preprocessed_pipe(headlines['clean_content'], batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enti"
   ]
  }
 ]
}